{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d10fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import orjson\n",
    "import ijson\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\")) # append calibration folder to path for McEval import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810cf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"output/Qwen3-Coder-30B-A3B/Scenario.codegeneration_10_0.2_eval_all.json\"\n",
    "out_path = \"output/Qwen3-Coder-30B-A3B/Scenario.codegeneration_10_0.2_eval_all.jsonl\"\n",
    "jsonlines = \"output/Qwen3-Coder-30B-A3B/codegeneration_10_0.2_eval_all.jsonl\"\n",
    "evaluation_data_path = \"output/Qwen3-Coder-30B-A3B/preprocessed/codegeneration_10_0.2.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30960531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# convert to jsonl\n",
    "def clean_logprobs(logprobs_list):\n",
    "    for sequence in logprobs_list:\n",
    "        for token in sequence:\n",
    "            for token_id in token:\n",
    "                token[token_id]['logprob'] = str(token[token_id]['logprob'])\n",
    "    return logprobs_list\n",
    "\n",
    "def convert_to_jsonl(data_path, out_path):\n",
    "    with open(data_path, \"rb\") as infile, open(out_path, \"wb\") as outfile:\n",
    "        for item in ijson.items(infile, \"item\"):  # iterate over each object in array\n",
    "            item['pass@1'] =str(item['pass@1'])\n",
    "            item['logprobs_list'] = clean_logprobs(item['logprobs_list'])\n",
    "            line = orjson.dumps(item) + b\"\\n\"\n",
    "            outfile.write(line)\n",
    "            \n",
    "# convert_to_jsonl(data_path, out_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8099589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Katarinas changes:\n",
    "\n",
    "\n",
    "def top20_logprob_sum_f(sample: dict):\n",
    "    top20 = []\n",
    "    for sequence in sample[\"logprobs_list\"]:\n",
    "        top20seq = []\n",
    "        count = 0  # count = Token-Nr. (beginnend mit 0)\n",
    "        for token in sequence:\n",
    "            top20seq.append(0)\n",
    "            for k in token.values():\n",
    "                if (k[\"rank\"] >= 1) and (k[\"rank\"] <= 20):\n",
    "                    top20seq[count] += float(k[\"logprob\"])\n",
    "            count += 1\n",
    "        top20.append(top20seq)\n",
    "    return top20\n",
    "\n",
    "\n",
    "def search(sample: dict):\n",
    "    # This is tokenizer-dependent\n",
    "    # Relevant tokens for Qwen3: 73594='```',\n",
    "    #                            12669='python',\n",
    "    #                            13874='``',\n",
    "    #                            19324='` '\n",
    "    #                            41233='Ġ```Ċ'\n",
    "\n",
    "    coding_tok_list = []\n",
    "    for sequence in sample[\"logprobs_list\"]:\n",
    "        token_list = []\n",
    "\n",
    "        index = 0\n",
    "        for token in sequence:\n",
    "            token_list.append(0)\n",
    "            for id, k in token.items():\n",
    "                if k[\"rank\"] == 1:\n",
    "                    token_list[index] = id\n",
    "            index += 1\n",
    "\n",
    "        position = []\n",
    "        for i in range(0, len(token_list) - 2):\n",
    "            if token_list[i] == \"73594\" and token_list[i + 1] == \"12669\":\n",
    "                if token_list[i + 2] == \"198\":\n",
    "                    position.append(i + 3)\n",
    "                else:\n",
    "                    position.append(i + 2)\n",
    "\n",
    "        if len(position) > 0:\n",
    "            position = position[\n",
    "                -1:\n",
    "            ]  # check this -> needs to handle incomplete last sequences\n",
    "\n",
    "            for j in range(position[0], len(token_list) - 1):\n",
    "                if token_list[j] == \"73594\":\n",
    "                    position.append(j - 1)\n",
    "                elif token_list[j] == \"13874\" and token_list[j + 1] == \"19324\":\n",
    "                    position.append(j - 1)\n",
    "\n",
    "        if len(position) > 2:\n",
    "            position = [\n",
    "                position[0]\n",
    "            ]  # Ignore end position if multiple are found. @Katharina: -> why?\n",
    "\n",
    "        coding_tok_list.append(position)\n",
    "\n",
    "    return coding_tok_list\n",
    "\n",
    "\n",
    "def coding_logprob_f(position_list, logprob_list):\n",
    "    coding_logprobs = {}\n",
    "    first_token = []\n",
    "    first_20_token = []\n",
    "    last_token = []\n",
    "    last_20_token = []\n",
    "\n",
    "    # if len(position_list)==len(logprob_list): #\n",
    "    assert len(position_list) == len(\n",
    "        logprob_list\n",
    "    ), \"Position list and logprob list need to be of equal size\"\n",
    "    for seq in range(0, len(logprob_list)):\n",
    "        seq_logprobs = logprob_list[seq]\n",
    "        seq_positions = position_list[seq]\n",
    "\n",
    "        if len(seq_positions) >= 1 and len(seq_positions) < 3:\n",
    "            first_token.append(seq_logprobs[(seq_positions[0])])\n",
    "\n",
    "            if (seq_positions[0] + 20) <= (len(seq_logprobs) - 1):\n",
    "                first_20_token.append(\n",
    "                    float(\n",
    "                        np.mean(\n",
    "                            seq_logprobs[(seq_positions[0]) : (seq_positions[0] + 20)]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                first_20_token.append([])\n",
    "\n",
    "            if len(seq_positions) == 2:\n",
    "                last_token.append(seq_logprobs[(seq_positions[1])])\n",
    "                last_20_token.append(\n",
    "                    float(\n",
    "                        np.mean(\n",
    "                            seq_logprobs[\n",
    "                                (seq_positions[1] - 19) : (seq_positions[1] + 1)\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                last_20_token.append([])\n",
    "                last_token.append([])\n",
    "\n",
    "        else:\n",
    "            first_token.append([])\n",
    "            first_20_token.append([])\n",
    "            last_20_token.append([])\n",
    "            last_token.append([])\n",
    "\n",
    "    coding_logprobs = {\n",
    "        \"first_token\": first_token,\n",
    "        \"first_20_token\": first_20_token,\n",
    "        \"last_token\": last_token,\n",
    "        \"last_20_token\": last_20_token,\n",
    "    }\n",
    "\n",
    "    return coding_logprobs\n",
    "\n",
    "\n",
    "def test(sample):\n",
    "    candidates = (\"73594\", \"12669\", \"13874\", \"19324\", \"41233\")\n",
    "\n",
    "    hit = []\n",
    "    for sequence in sample[\"logprobs_list\"]:\n",
    "        hit_seq = []\n",
    "        index = 0  # count = Token-Nr. (beginnend mit 0)\n",
    "        for token in sequence:\n",
    "            for id, k in token.items():\n",
    "                if (\n",
    "                    k[\"rank\"] == 1\n",
    "                    and (k[\"decoded_token\"].find(\"``\") != -1)\n",
    "                    and id not in candidates\n",
    "                ):\n",
    "                    hit_seq.append([id, k[\"decoded_token\"], index])\n",
    "\n",
    "            index += 1\n",
    "        if len(hit_seq) > 0:\n",
    "            hit.extend(hit_seq)\n",
    "    if len(hit) > 0:\n",
    "        print(f\"Check sample {sample['question_title']}: {hit}\")\n",
    "\n",
    "\n",
    "def prepare_sample(sample: dict):\n",
    "    logprobs = [\n",
    "        [\n",
    "            k[\"logprob\"]\n",
    "            for d in sequence\n",
    "            for k in filter(lambda x: x[\"rank\"] == 1, d.values())\n",
    "        ]\n",
    "        for sequence in sample[\"logprobs_list\"]\n",
    "    ]\n",
    "\n",
    "    code_token_idxs = [extract_code_tokens(seq) for seq in sample[\"logprobs_list\"]]\n",
    "\n",
    "    logprob_sum = top20_logprob_sum_f(sample)\n",
    "    average_top20 = [[x / 20 for x in seq] for seq in logprob_sum]\n",
    "    avg_top20_code_probs = [\n",
    "        float(np.mean(seq[start:stop]))\n",
    "        for seq, [start, stop] in zip(average_top20, code_token_idxs)\n",
    "    ]\n",
    "    code_logprobs = [\n",
    "        float(np.mean(np.array(seq[start:stop], dtype=float)))\n",
    "        for seq, [start, stop] in zip(logprobs, code_token_idxs)\n",
    "    ]\n",
    "    avg_top20_tail = [float(np.mean(seq[-40:])) for seq in average_top20]\n",
    "    tail_logprobs = [float(np.mean(np.array(seq[-40:], dtype=float))) for seq in logprobs]\n",
    "    # coding_tok_pos = search(sample)\n",
    "    coding_log_probs = coding_logprob_f(code_token_idxs, logprob_sum)\n",
    "\n",
    "    # test(sample)\n",
    "\n",
    "    return {\n",
    "        \"id\": sample[\"question_id\"],\n",
    "        \"name\": sample[\"question_title\"],\n",
    "        \"prompt\": sample[\"question_content\"],\n",
    "        \"program\": sample[\"code_list\"],\n",
    "        \"language\": \"python\",\n",
    "        \"output_size\": [len(output) for output in sample[\"output_list\"]],\n",
    "        \"token_count\": [len(logprobs) for logprobs in sample[\"logprobs_list\"]],\n",
    "        \"is_correct\": sample[\"graded_list\"],\n",
    "        \"token_logprobs\": logprobs,\n",
    "        \"avg_top20\": average_top20,\n",
    "        \"avg_top20_code_probs\": avg_top20_code_probs,\n",
    "        \"avg_top20_tail\": avg_top20_tail,\n",
    "        \"code_logprob\": code_logprobs,\n",
    "        \"tail_logprob\": tail_logprobs,\n",
    "        \"cumulative_logprob\": [sum([float(x) for x in l]) for l in logprobs],\n",
    "        \"code_token_idx\": code_token_idxs,\n",
    "        \"top_logprobs_first_token\": [sequence[0] for sequence in logprob_sum],\n",
    "        \"top_logprobs_first_20_tokens\": [\n",
    "            (float(np.mean(sequence[0:20]))) for sequence in logprob_sum\n",
    "        ],\n",
    "        \"top_logprobs_last_token\": [sequence[-1] for sequence in logprob_sum],\n",
    "        \"top_logprobs_last_20_tokens\": [\n",
    "            (float(np.mean(sequence[(len(sequence) - 20) : (len(sequence))])))\n",
    "            for sequence in logprob_sum\n",
    "        ],\n",
    "        \"top_logprobs_first_coding_token\": coding_log_probs[\"first_token\"],\n",
    "        \"top_logprobs_first_20_coding_tokens\": coding_log_probs[\"first_20_token\"],\n",
    "        \"top_logprobs_last_coding_tokens\": coding_log_probs[\"last_token\"],\n",
    "        \"top_logprobs_last_20_coding_tokens\": coding_log_probs[\"last_20_token\"],\n",
    "        \"difficulty\": sample[\"difficulty\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b901328",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a5a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_rank(prob_entry:dict, rank:Optional[int]=1) -> bool:\n",
    "    return prob_entry['rank']==rank\n",
    "\n",
    "def extract_code_tokens(probs_list:List[dict]) -> List[dict]:\n",
    "    \"\"\"Extract tokens and probabilities for the last ```python ... ``` block.\n",
    "        param probs_list: list of token probs for 1 generation\n",
    "    \"\"\"\n",
    "    \n",
    "    top_tokens = [x for entry in probs_list for x in filter(is_rank, entry.values())]\n",
    "    tokens = [t['decoded_token'] for t in top_tokens]\n",
    "    \n",
    "    # Join tokens into a single string to find indices\n",
    "    joined = \"\".join(tokens)\n",
    "    start_marker = \"```python\"\n",
    "    end_marker = \"```\"\n",
    "\n",
    "    # Find last start and end indices in the joined string\n",
    "    start_idx = joined.rfind(start_marker)\n",
    "    if start_idx == -1:\n",
    "        return [0,0] # None, None, []\n",
    "\n",
    "    end_idx = joined.find(end_marker, start_idx + len(start_marker))\n",
    "    if end_idx == -1:\n",
    "        return [0,0] # start_idx, None, []\n",
    "\n",
    "    # Map character indices back to token indices\n",
    "    cumulative = 0\n",
    "    start_token, end_token = None, None\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if start_token is None and cumulative >= start_idx:\n",
    "            start_token = i\n",
    "        if cumulative >= end_idx:\n",
    "            end_token = i\n",
    "            break\n",
    "        cumulative += len(tok)\n",
    "\n",
    "    # Slice token/probability lists\n",
    "    if start_token is not None and end_token is not None:\n",
    "        return [start_token, end_token] #, top_tokens[start_token:end_token]\n",
    "    else:\n",
    "        return [0,0] #None, None, []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f602f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(input_path, output_path, max_samples=None):\n",
    "    i = 0\n",
    "    with open(input_path, \"rb\") as infile, open(output_path, \"wb\") as outfile:\n",
    "        for line in tqdm(infile):\n",
    "            if max_samples and i>=max_samples:\n",
    "                break\n",
    "            obj = orjson.loads(line)\n",
    "            sample = prepare_sample(obj)\n",
    "            line = orjson.dumps(sample) + b\"\\n\"\n",
    "            outfile.write(line)\n",
    "            i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4dafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:55, 55.91s/it]/data/dok/viola/projects/calibration/LiveCodeBench/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/data/dok/viola/projects/calibration/LiveCodeBench/.venv/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "2it [01:39, 48.68s/it]/data/dok/viola/projects/calibration/LiveCodeBench/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/data/dok/viola/projects/calibration/LiveCodeBench/.venv/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "3it [01:51, 31.97s/it]/data/dok/viola/projects/calibration/LiveCodeBench/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/data/dok/viola/projects/calibration/LiveCodeBench/.venv/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "4it [02:02, 23.71s/it]/data/dok/viola/projects/calibration/LiveCodeBench/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/data/dok/viola/projects/calibration/LiveCodeBench/.venv/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "10it [02:07, 12.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# Full preprocessing pipeline for LiveCodeBench\n",
    "\n",
    "# Qwen3-Coder\n",
    "in_path_qwen3 = \"output/Qwen3-Coder-30B-A3B/Scenario.codegeneration_10_0.2_eval_all.json\"\n",
    "jsonl_path_qwen3 = \"output/Qwen3-Coder-30B-A3B/Scenario.codegeneration_10_0.2_eval_all.jsonl\"\n",
    "out_path_qwen3 = \"output/Qwen3-Coder-30B-A3B/preprocessed/codegeneration_10_0.2.jsonl\"\n",
    "\n",
    "# gpt-oss\n",
    "in_path_gpt_oss = \"output/GPT-OSS-20B/Scenario.codegeneration_10_0.2_eval_all.json\"\n",
    "jsonl_path_gpt_oss = \"output/GPT-OSS-20B/codegeneration_10_0.2_eval_all.jsonl\"\n",
    "out_path_gpt_oss = \"output/GPT-OSS-20B/preprocessed/codegeneration_10_0.2.jsonl\"\n",
    "\n",
    "\n",
    "def prepare_lcb_for_evaluation(in_path, jsonl_path, out_path):\n",
    "    convert_to_jsonl(in_path, jsonl_path)\n",
    "    preprocess_data(jsonl_path, out_path)\n",
    "    \n",
    "# prepare_lcb_for_evaluation(in_path=in_path_gpt_oss,\n",
    "#                            jsonl_path=jsonl_path_gpt_oss,\n",
    "#                            out_path=out_path_gpt_oss)\n",
    "\n",
    "# prepare_lcb_for_evaluation(in_path=in_path_qwen3,\n",
    "#                            jsonl_path=jsonl_path_qwen3,\n",
    "#                            out_path=out_path_qwen3)\n",
    "\n",
    "preprocess_data(jsonl_path_gpt_oss, out_path_gpt_oss) #, max_samples=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf46b1",
   "metadata": {},
   "source": [
    "### Groups\n",
    "- loc\n",
    "- cyclomatic complexity\n",
    "- len output\n",
    "- len code\n",
    "- difficulty\n",
    "- len prompt\n",
    "- language\n",
    "\n",
    "(identifier count, etc)\n",
    "\n",
    "### Metrics\n",
    "- mean logprob\n",
    "@ Katharina:\n",
    "- sum top-20 logprobs (last n tokens, last token, last code token, first token)\n",
    "     1) last 20 tokens (Konfigurierbar für n)\n",
    "     2) last token\n",
    "     3) first 20 tokens\n",
    "     4) last 20 code tokens\n",
    "     5) last code token\n",
    "     6) first code token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiveCodeBench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
